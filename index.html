<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>

  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  
  /* 论文区的 bibtex 代码块 *//* 仅作用于论文区的 bibtex 代码块 */
  .paper pre {
    display: none;              /* 初始隐藏，配合你的 toggle 使用 */
    max-width: 100%;            /* 不允许超过单元格宽度 */
    overflow-x: auto;           /* 太长时出横向滚动条，而不是撑开布局 */
    white-space: pre-wrap;      /* 允许按空白换行 */
    word-break: break-word;     /* 必要时断长单词 */
    box-sizing: border-box;

    /* 下面是观感优化，可要可不要 */
    background: #f7f7f7;
    border-radius: 8px;
    padding: 10px 12px;
    margin: 8px 0 0;
    font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace;
    font-size: 13px;
    line-height: 1.4;
  }

  </style>
  <!-- <link rel="shortcut icon" href="images/apple-touch-ri-logo-white-120x120.png"> -->
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Huang Yinming</title>
  <meta name="Yinming Huang's Homepage" http-equiv="Content-Type" content="Yinming Huang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>


</head>
 
<body>
  
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Yinming Huang 「黄音铭」</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/yinminghuang_Cal.jpg"><img src="images/yinminghuang_Cal.jpg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="https://github.com/YinmingHuang">Github</a> |
    <a href="mailto:yinminghuang22@m.fudan.edu.cn">Email</a> |
    <a href="https://scholar.google.com/citations?hl=en&user=EVNtdFIAAAAJ">Google Scholar</a> |
    <p align="center" style="font-family: 'Titillium Web', sans-serif; font-size: 15px; font-style: italic; color: #666; margin-top: -10px;">
      "Be patient, then doctor."
    </p>
    
    <!-- <a href="https://scholar.google.com/citations?user=TVWH2U8AAAAJ">Google Scholar</a> |
    <br/>
    | <a href="https://github.com/TairanHe">Github</a> | 
    <a href="https://www.linkedin.com/in/tairan-he-41a904294/">LinkedIn</a> |
    <a href="https://space.bilibili.com/14145636">Bilibili</a> |  -->
    <!-- </p>
    <p align="center" style="margin-top:-8px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 156px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=TairanHe99&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1706734206165" data-screen-name=""></iframe><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p> -->
    </td>
    <td width="70%" valign="top" align="justify">
      <p>
        I'm currently a fourth-year undergraduate student at 
        <a href="https://www.fudan.edu.cn/">Fudan University</a>, majoring in Artificial Intelligence. 
        Starting Fall 2026, I will pursue my Ph.D. jointly at <a href="https://fvl.fudan.edu.cn/">Fudan Vision and Learning Laboratory</a>, <a href="https://www.fudan.edu.cn/">Fudan University</a> and 
        <a href="https://www.sii.edu.cn/">Shanghai Innovation Institute</a>, advised by 
        <a href="https://zxwu.azurewebsites.net/">Prof. Zuxuan Wu</a>.
        </p>
        
        <p>
        My research interests lie in Unified Models for both Understanding and Generation, 
        with a focus on Generative AI and Multimodal Intelligence.
        </p>
        
        
      <!-- <p>Goal: challenge conventional notions of what robots can achieve, develop robots that improves everyone's life. Focus: developing intelligent robots being able to do useful tasks with <u>intelligence, generalizability, agility and safety</u>. Method: learning-based methods that scale with the computation and data. Robots: Mobile robots, legged robots, robotic manipulators, and humanoid robots.
      </p> -->
      <!-- <p><strong>Goal:</strong> Robots that improve everyone's life.</p>
      <p><strong>Focus:</strong> How to build the <u>data flywheel for robotics</u> to unlock human-level athletic skills and semantic intelligence? How to make robots perform useful tasks with <u>adaptability, generalizability, agility, and safety</u>?</p>
      <p><strong>Method:</strong> Utilizing learning-based methods that scale with computation and data.</p>
      <p><strong>Robots:</strong> I love working on humanoids and aim to make them capable of doing everything I can do—and more.</p> -->
      <p>Email: 22307140123 [AT] m.fudan.edu.cn</p>
    </td>
  </tr>
</table>


<hr/>
<!-- ===================== Education, Awards, Experience Section ===================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr valign="top">
    <!-- ========== Left Column ========== -->
    <td width="50%">
      <sectionheading>Education</sectionheading>
      <p>
        <strong>Fudan University</strong><br/>
        B.Eng. in Artificial Intelligence<br/>
        <em>Sep. 2022 – Present</em>
      </p>
      <p>
        <strong>No. 2 High School Attached to East China Normal University</strong><br/>
        High School Diploma<br/>
        <em>Aug. 2019 – Jul. 2022</em>
      </p>

      <sectionheading>Honors & Awards</sectionheading>
      <ul>
        <li>First-Class Scholarship for Outstanding Undergraduate Students, Fudan University, 2023-2024</li>
        <li>First-Class Scholarship for Outstanding Undergraduate Students, Fudan University, 2022-2023</li>
        <li>First Prize in the Preliminary Round of the Chinese Mathematical Olympiad (2021)</li>
      </ul>
    </td>

    <!-- ========== Right Column ========== -->
    <td width="50%">
      <sectionheading>Experience</sectionheading>
      <p>
        <strong>2030 Lab, Yinwang Intelligent Technology Co., Ltd.</strong><br/>
        Research Intern<br/>
        <em>Dec. 2025 – Present</em>
      </p>
      <p>
        <strong>University of California, San Diego (UCSD)</strong><br/>
        Exchange Student<br/>
        <em>Sep. 2024 – Dec. 2024</em>
      </p>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">


  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://francis-rings.github.io/StableAvatar/">
          <img src="images/stableavatar/framework_00.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://francis-rings.github.io/StableAvatar/" id="STABLEAVATAR">
      <heading>StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation  </heading></a><a class="github-button"
      href="https://github.com/Francis-Rings/StableAvatar"
      data-icon="octicon-star"
      data-show-count="true"
      aria-label="Star Francis-Rings/StableAvatar on GitHub">
     Star
   </a>
   <br>
      Shuyuan Tu, Yueming Pan, <strong>Yinming Huang</strong>, Xintong Han, Zhen Xing, Qi Dai, Chong Luo, Zuxuan Wu, Yu-Gang Jiang<br>
      </p>

      <div class="paper" id="stableavatar">
      <a href="https://francis-rings.github.io/StableAvatar/">webpage</a> |
      <a href="javascript:toggleblock('stableavatar_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('stableavatar')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2508.08248">arXiv</a> |
      <a href="https://github.com/Francis-Rings/StableAvatar">code</a> |
      <a href="https://www.youtube.com/watch?v=6lhvmbzvv3Y">YouTube</a> |
      <a href="https://www.bilibili.com/video/BV1hUt9z4EoQ/?vd_source=4f0cebca1f5f6e82beeaeb683fe509c5">Bilibili</a> |
      <a href="https://mp.weixin.qq.com/s/BoHk9XZRdaSGMSK-9_PpGA">机器之心</a> |
      <a href="https://huggingface.co/spaces/YinmingHuang/StableAvatar">online demo</a>
      

      <p align="justify" id="stableavatar_abs" style="display: none;"> <i>Current diffusion models for audio-driven avatar video generation struggle to synthesize long videos with natural audio synchronization and identity consistency. This paper presents StableAvatar, the first end-to-end video diffusion transformer that synthesizes infinite-length high-quality videos without post-processing. Conditioned on a reference image and audio, StableAvatar integrates tailored training and inference modules to enable infinite-length video generation. We observe that the main reason preventing existing models from generating long videos lies in their audio modeling. They typically rely on third-party off-the-shelf extractors to obtain audio embeddings, which are then directly injected into the diffusion model via cross-attention. Since current diffusion backbones lack any audio-related priors, this approach causes severe latent distribution error accumulation across video clips, leading the latent distribution of subsequent segments to drift away from the optimal distribution gradually. To address this, StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents error accumulation via time-step-aware modulation. During inference, we propose a novel Audio Native Guidance Mechanism to further enhance the audio synchronization by leveraging the diffusion's own evolving joint audio-latent prediction as a dynamic guidance signal. To enhance the smoothness of the infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy that fuses latent over time. Experiments on benchmarks show the effectiveness of StableAvatar both qualitatively and quantitatively.</i></p>

      <tr>
        <td width="40%" valign="top" align="center">
          <a href="https://github.com/Francis-Rings/FlashPortrait/">
              <img src="images/flashportrait/framework.jpg" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
          </a>
        </td>
        <td width="60%" valign="top">
          <p><a href="https://francis-rings.github.io/FlashPortrait/" id="FLASHPORTAIT">
          <heading>FlashPortrait: 6$\times$ Faster Infinite Portrait Animation with Adaptive Latent Prediction  </heading></a><a class="github-button"
          href="https://github.com/Francis-Rings/FlashPortrait"
          data-icon="octicon-star"
          data-show-count="true"
          aria-label="Star Francis-Rings/FlashPortrait on GitHub">
         Star
       </a>
       <br>
          Shuyuan Tu, Yueming Pan, <strong>Yinming Huang</strong>, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu, Yu-Gang Jiang<br>
          </p>
    
          <div class="paper" id="flashportrait">
          <a href="https://francis-rings.github.io/FlashPortrait/">webpage</a> |
          <a href="javascript:toggleblock('flashportrait_abs')">abstract</a> |
          <a shape="rect" href="javascript:togglebib('flashportrait')" class="togglebib">bibtex</a> |
          <a href="https://arxiv.org/abs/2512.16900">arXiv</a> |
          <a href="https://github.com/Francis-Rings/FlashPortrait">code</a> |
          <a href="https://www.youtube.com/watch?v=woSzRXlXyiY">YouTube</a> |
          <a href="https://www.bilibili.com/video/BV1Gfq9BAEvo/?vd_source=4f0cebca1f5f6e82beeaeb683fe509c5">Bilibili</a> |
          
    
          <p align="justify" id="flashportrait_abs" style="display: none;"> <i>Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6$\times$ acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6$\times$ speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.</i></p>

<pre xml:space="preserve" style="display: none;">
  @article{tu2025flashportrait,
    title={FlashPortrait: 6$\times$ Faster Infinite Portrait Animation with Adaptive Latent Prediction},
    author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Qiu, Kai and Luo, Chong and Wu, Zuxuan},
    journal={arXiv preprint arXiv:2512.16900},
    year={2025}
  }
</pre>
      </div>
    </td>
  </tr>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="https://tairanhe.com/">here</a> and <a href="https://github.com/luost26/academic-homepage">here</a> 
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>

</body>

</html>
